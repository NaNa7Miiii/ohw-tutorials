{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51f2ac61",
   "metadata": {},
   "source": [
    "## Supervised learning\n",
    "\n",
    "We have the human operator labels for our data, so we can train models to predict these labels instead of using them only to check our model.\n",
    "\n",
    "Examples of supervised learning include linear regression, support vector machines, neural networks (including deep learning), and more.\n",
    "\n",
    "The simplest possible supervised learning model for our classification problems is a [k-nearest-neighbors classifier](https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification) (kNN).\n",
    "\n",
    "kNN assigns labels based on the labels of a sample's *k* nearest neighbors. Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ff44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"pellets-visual-classes-rgb.csv\", index_col=\"image\").dropna()\n",
    "df[\"yellowing index\"] = df[\"yellowing index\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53baa2ff",
   "metadata": {},
   "source": [
    "We already know that the size is mostly random so let's drop it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b0d005",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = [\"r\", \"g\", \"b\"]\n",
    "X = df[feature_columns].values\n",
    "\n",
    "y = df[\"yellowing\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bd423e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(X, y)\n",
    "prediction = knn.predict(X)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426ffca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.accuracy_score(y, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e749e2",
   "metadata": {},
   "source": [
    "Quite the improvement from our k-means attempt. Aren't we forgeting anything though? Yes, we should always standardize the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78917add",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608d5f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier()\n",
    "knn.fit(X_scaled, y)\n",
    "prediction_scaled = knn.predict(X_scaled)\n",
    "\n",
    "metrics.accuracy_score(y, prediction_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2081f46",
   "metadata": {},
   "source": [
    "The lower score means that we where overfitting before standardizing. Still, ~78% is much better than our k-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95363299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "redux = df[[\"r\", \"g\", \"b\", \"yellowing\"]]\n",
    "redux = redux.assign(knn=prediction_scaled)\n",
    "\n",
    "sns.pairplot(redux, hue=\"knn\", vars=feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d1a49c",
   "metadata": {},
   "source": [
    "How can we stop forgetting to standardize the data? Well, scikit-learn is awesome and has our back. We can create data processing pipelines and keep all the steps of our model in a single object. Pipelines are very robust and may contain custom steps if your data requires them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93feeab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import pipeline\n",
    "\n",
    "classifier = pipeline.make_pipeline(\n",
    "    preprocessing.StandardScaler(),\n",
    "    neighbors.KNeighborsClassifier(),\n",
    ")\n",
    "\n",
    "classifier.fit(X, y)\n",
    "prediction_pipeline = classifier.predict(X)\n",
    "\n",
    "metrics.accuracy_score(y, prediction_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e03fea",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2284f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "split = model_selection.train_test_split(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = split\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67b487",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0edd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, classifier.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a71074",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_selection.cross_val_score(classifier, X, y)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45209b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0a9e78",
   "metadata": {},
   "source": [
    "We reduce our accuracy when performing a test/train split. Why that happened? The first guess is that our model may be \"data hungry.\" We just don't have enough samples on each class to predict them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2caa8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"yellowing\"].value_counts().plot.barh(title=\"yellowing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee4942",
   "metadata": {},
   "source": [
    "The data is balanced! That can we do next?\n",
    "\n",
    "- Try to balance the current data;\n",
    "- Collect more data and see if the dataset balance itself out;\n",
    "- Choose a technique that is more robust to unbalanced data, like Decision Trees (DT).\n",
    "\n",
    "PS: Check this [awesome paper](https://hal.archives-ouvertes.fr/hal-03723551) on DTs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
