{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkg6Zjm-bHRD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# InputEmbedding class creates an embedding layer that scales the output embeddings by the square root of the embedding dimension (d_model).\n",
        "# This scaling helps stabilize gradients during training.\n",
        "class InputEmbedding(nn.Module):\n",
        "  def __init__(self, d_model:int, vocab_size:int):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "  def forward(self, x):\n",
        "    return self.embedding(x) * math.sqrt(self.d_model)"
      ],
      "metadata": {
        "id": "us0QDh0qbnlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PositionalEncoding class generates and applies sinusoidal positional encodings to the input embeddings.\n",
        "# This encoding helps the model capture the order of tokens in a sequence, which is crucial for sequential data processing.\n",
        "# The dropout layer is used to prevent overfitting by randomly zeroing some of the elements in the input tensor.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        positional_encoding = torch.zeros(seq_len, d_model)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        positional_encoding[:, 0::2] = torch.sin(position * div_term)\n",
        "        positional_encoding[:, 1::2] = torch.cos(position * div_term)\n",
        "        positional_encoding = positional_encoding.unsqueeze(0)\n",
        "        self.register_buffer('positional_encoding', positional_encoding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.positional_encoding[:, :x.shape[1], :].requires_grad_(False)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "RwhhUcf-dCvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LayerNormalization class applies layer normalization to the input tensor.\n",
        "# Layer normalization stabilizes the learning process by normalizing the input across the features of a single layer,\n",
        "# ensuring that the outputs have zero mean and unit variance. This is particularly useful in deep networks to prevent\n",
        "# internal covariate shift.\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, eps: float = 10**-6) -> None:\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.alpha = nn.Parameter(torch.ones(1))\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.alpha * (x - mean) / (std + self.eps) + self.bias"
      ],
      "metadata": {
        "id": "eheFwnYpj5j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FeedForwardBlock class implements a two-layer feedforward neural network with ReLU activation and dropout.\n",
        "# This block is typically used in transformer models to process the output of the attention mechanism.\n",
        "# The first linear layer expands the dimensionality, the ReLU activation adds non-linearity,\n",
        "# dropout is applied for regularization, and the second linear layer projects the output back to the original dimension.\n",
        "class FeedForwardBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(d_model, d_ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear_2(self.dropout(F.relu(self.linear_1(x))))"
      ],
      "metadata": {
        "id": "tacXAXfblend"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MultiHeadAttentionBlock class implements the multi-head attention mechanism used in transformer models.\n",
        "# Multi-head attention allows the model to focus on different parts of the input sequence simultaneously,\n",
        "# capturing various relationships between tokens. The input is split into multiple heads, each head performs\n",
        "# scaled dot-product attention, and the results are concatenated and projected back to the original dimension.\n",
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.h = h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "        self.d_k = d_model // h\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def attention(self, query, key, value, mask, dropout):\n",
        "        d_k = query.shape[-1]\n",
        "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_scores = attention_scores.softmax(dim=-1)\n",
        "        if dropout is not None:\n",
        "            attention_scores = dropout(attention_scores)\n",
        "        return torch.matmul(attention_scores, value)\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "        query = self.w_q(q)\n",
        "        key = self.w_k(k)\n",
        "        value = self.w_v(v)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1, 2)\n",
        "        x = self.attention(query, key, value, mask, self.dropout)\n",
        "        x = x.transpose(1, 2).contiguous().reshape(x.shape[0], -1, self.h * self.d_k)\n",
        "        return self.w_o(x)"
      ],
      "metadata": {
        "id": "YmyWXrn-mUkQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Residual class implements a residual connection with layer normalization and dropout.\n",
        "# Residual connections help in training deep networks by mitigating the vanishing gradient problem,\n",
        "# allowing gradients to flow through the network more effectively.\n",
        "# The input is first normalized, passed through a sublayer, followed by dropout,\n",
        "# and finally added back to the original input.\n",
        "class Residual(nn.Module):\n",
        "    def __init__(self, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        return x + self.dropout(sublayer(self.norm(x)))"
      ],
      "metadata": {
        "id": "hCh2QZ0c6Uke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EncoderBlock class represents a single block in the transformer encoder.\n",
        "# It consists of a multi-head self-attention mechanism, followed by a feedforward network,\n",
        "# with residual connections and layer normalization applied after each sublayer.\n",
        "# This structure allows the encoder to capture complex relationships in the input sequence while maintaining stable gradients.\n",
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([Residual(dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RD1J2-5pALNy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder class stacks multiple encoder blocks to form the transformer encoder.\n",
        "# The input passes through each encoder block in sequence, allowing the model to build a rich representation of the input.\n",
        "# Finally, layer normalization is applied to the output of the last encoder block for stability.\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList) -> None:\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "Hzt5OhRUALth"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 5  # vocabulary size example\n",
        "d_model = 512\n",
        "seq_len = 5  # sequence length example\n",
        "\n",
        "# test example\n",
        "vocab = {word: idx for idx, word in enumerate([\"this\", \"is\", \"an\", \"example\", \"sentence\"])}\n",
        "sentence = [\"this\", \"is\", \"an\", \"example\", \"sentence\"]\n",
        "\n",
        "# convert the sentence into indices\n",
        "input_indices = [vocab[word] for word in sentence]\n",
        "\n",
        "input_tensor = torch.LongTensor(input_indices).unsqueeze(0)  # (1, seq_len)\n",
        "\n",
        "# create embedding and positional encoding\n",
        "embedding = InputEmbedding(d_model=d_model, vocab_size=vocab_size)\n",
        "pos_encoding = PositionalEncoding(d_model=d_model, seq_len=seq_len, dropout=0.1)\n",
        "\n",
        "# create encoder layers\n",
        "num_layers = 6\n",
        "dropout = 0.1\n",
        "attention_heads = 8\n",
        "d_ff = 2048\n",
        "\n",
        "layers = nn.ModuleList([\n",
        "    EncoderBlock(\n",
        "        MultiHeadAttentionBlock(d_model=d_model, h=attention_heads, dropout=dropout),\n",
        "        FeedForwardBlock(d_model=d_model, d_ff=d_ff, dropout=dropout),\n",
        "        dropout=dropout\n",
        "    ) for _ in range(num_layers)\n",
        "])\n",
        "\n",
        "encoder = Encoder(layers=layers)\n",
        "\n",
        "# feed forward\n",
        "x = embedding(input_tensor)\n",
        "x = pos_encoding(x)\n",
        "output = encoder(x, None)\n",
        "\n",
        "print(output)\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OtHx6tgNMWR",
        "outputId": "a6201993-30ac-43cd-f6ac-071d09d3045e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-4.7664e-04,  3.5593e-01, -1.0730e+00,  ..., -1.5652e+00,\n",
            "          -9.0137e-01,  7.7596e-01],\n",
            "         [-4.8475e-01,  7.7966e-01, -1.0795e-02,  ..., -2.6110e-01,\n",
            "           5.0710e-01,  4.4744e-01],\n",
            "         [-1.2681e-01,  1.8640e-01, -1.5022e+00,  ...,  2.3316e+00,\n",
            "          -1.5395e-01,  7.0094e-01],\n",
            "         [-6.8704e-01,  1.4763e-02,  1.5459e-01,  ..., -9.5795e-01,\n",
            "           5.3138e-01, -4.1171e-01],\n",
            "         [-2.0745e+00,  3.8764e-01,  1.1710e-02,  ...,  7.0964e-01,\n",
            "           2.4866e-01, -4.2348e-01]]], grad_fn=<AddBackward0>)\n",
            "torch.Size([1, 5, 512])\n"
          ]
        }
      ]
    }
  ]
}