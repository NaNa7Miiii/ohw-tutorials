{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how to run a simple deep learning model to classify whale calls. We use the [keras](https://keras.io/) library. We will use a convolutional neural networks, since they are robust to temporal and spatial shifts. Although it is common to use 2D convolutional model on the spectrogram, we will use a 1D on the time series, as this can be applied to other time series classification problems. But instead of raw data, we will use Welch-filtered signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing multiple visualization libraries\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import mlab\n",
    "import pylab as pl\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries to manipulate the data files\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a library to read the .aiff format\n",
    "import aifc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob(os.path.join('../../../whale_data','train','*.aiff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read signals and apply the welch filter\n",
    "feature_dict = {}\n",
    "fs = 2000\n",
    "for filename in filenames[:10000]:\n",
    "    aiff = aifc.open(filename,'r')\n",
    "    whale_strSig = aiff.readframes(aiff.getnframes())\n",
    "    whale_array = np.fromstring(whale_strSig, np.short).byteswap()\n",
    "    # apply welch filter\n",
    "    feature = 10*np.log10(signal.welch(whale_array, fs=fs, window='hanning', nperseg=256, noverlap=128+64)[1])\n",
    "    feature_dict[filename] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "X = pd.DataFrame(feature_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Deep learning on time domain samples.\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv1D, GlobalAveragePooling1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, CSVLogger, TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = np.array(labels['label'][X.columns])\n",
    "y = np.load('y.npy')\n",
    "y = y.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000, 129)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "target_names = ['Upcall', 'NO_Upcall']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=2018)\n",
    "\n",
    "# Convert label to onehot\n",
    "#y_train = keras.utils.to_categorical(y_train, num_classes=2)\n",
    "#y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train = np.expand_dims(X_train, axis=2)\n",
    "X_test = np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Neural Network\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(16, 3, activation='relu', input_shape=(129, 1)))\n",
    "model.add(Conv1D(16, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Conv1D(32, 3, activation='relu'))\n",
    "model.add(Conv1D(32, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(Conv1D(64, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(Conv1D(128, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(256, 3, activation='relu'))\n",
    "model.add(Conv1D(256, 3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=SGD(lr=0.01),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_name = 'deep_1'\n",
    "top_weights_path = 'model_' + str(model_name) + '.h5'\n",
    "\n",
    "callbacks_list = [ModelCheckpoint(top_weights_path, monitor = 'val_acc', verbose = 1, save_best_only = True, save_weights_only = True), \n",
    "    EarlyStopping(monitor = 'val_acc', patience = 6, verbose = 1),\n",
    "    ReduceLROnPlateau(monitor = 'val_acc', factor = 0.1, patience = 3, verbose = 1),\n",
    "    CSVLogger('model_' + str(model_name) + '.log')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a lot of jargon:\n",
    "\n",
    "|Term| Explanation|\n",
    "|---|---|\n",
    "|Convolutional Layers | layers which are robust to time shifts|\n",
    "|Max Pooling/ Average Pooling |dimension reduction, robustness|\n",
    "|Batch normalization |equalizes the distribution of the batches |\n",
    "|Epochs |1 run of a batch |\n",
    "|Adam Optimizer | an adaptive optimization scheme|\n",
    "|Cross Entropy | cost function|\n",
    "|ReLU | nonlinear activation function| \n",
    "|Batch Size| size of subset to process to update the estimates|\n",
    "|Learning Rate | time step of the optimization algorithm|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from keras.utils import plot_model\n",
    "#plot_model(model,to_file='model_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/model_plot.png\" width=200/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 8s 941us/step - loss: 0.6437 - acc: 0.6564 - val_loss: 0.3892 - val_acc: 0.8260\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.82600, saving model to model_deep_1.h5\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 5s 682us/step - loss: 0.4913 - acc: 0.7875 - val_loss: 0.3910 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.82600 to 0.84950, saving model to model_deep_1.h5\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 6s 698us/step - loss: 0.4130 - acc: 0.8371 - val_loss: 0.3654 - val_acc: 0.8545\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.84950 to 0.85450, saving model to model_deep_1.h5\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 6s 708us/step - loss: 0.3711 - acc: 0.8563 - val_loss: 0.3371 - val_acc: 0.8650\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.85450 to 0.86500, saving model to model_deep_1.h5\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 6s 728us/step - loss: 0.3494 - acc: 0.8619 - val_loss: 0.3016 - val_acc: 0.8775\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.86500 to 0.87750, saving model to model_deep_1.h5\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 6s 732us/step - loss: 0.3269 - acc: 0.8700 - val_loss: 0.3186 - val_acc: 0.8705\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.87750\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 6s 787us/step - loss: 0.3099 - acc: 0.8732 - val_loss: 0.3062 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.87750\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 6s 784us/step - loss: 0.2984 - acc: 0.8749 - val_loss: 0.2843 - val_acc: 0.8830\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fitting the Model (this will take a loooooooot of time)\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=100, validation_data = [X_test, y_test], callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(top_weights_path)\n",
    "loss, acc = model.evaluate(X_test, y_test, batch_size=16)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision\n",
    "sum(y_pred[y_test == 1]>.1)/len(y_test == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import average_precision_score\n",
    "average_precision = average_precision_score(y_test.astype('int'), y_pred)\n",
    "\n",
    "print('Average precision-recall score: {0:0.2f}'.format(\n",
    "      average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test.astype('int'), y_pred)\n",
    "\n",
    "plt.step(recall, precision, color='b', alpha=0.2,\n",
    "         where='post')\n",
    "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
    "                 color='b')\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.title('2-class Precision-Recall curve: AP={0:0.2f}'.format(\n",
    "          average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References:**\n",
    "\n",
    "[Deep Learning Glossary](http://www.wildml.com/deep-learning-glossary/)\n",
    "\n",
    "[Keras and NN Tutorial](https://indico.cern.ch/event/506145/contributions/2132944/attachments/1258124/1858154/NNinKeras_MPaganini.pdf)\n",
    "\n",
    "[Keras Cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Keras_Cheat_Sheet_Python.pdf)\n",
    "\n",
    "Free GPU usage: [Google Colaboratory notebooks](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true) & [Kaggle Kernels](https://www.kaggle.com/kernels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
