{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Whale Sound Exploration\n",
    "\n",
    "In this tutorial we will explore some data which contain right whale up-calls. The dataset was shared as part of a [2013 Kaggle competition](https://www.kaggle.com/c/whale-detection-challenge). Our goal is not to show the best winning algorithm to detect a call, but share a simple pipeline for processing oscillatory data, which possibly can be used on wide range of time series.\n",
    "\n",
    "Objectives:\n",
    "* read and extract features form audio data\n",
    "* apply dimensionality reduction techiques\n",
    "* perform supervised classification\n",
    "* learn how to evaluate machine learning models\n",
    "* train a neural network to detect whale calls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Loading and Exploration\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# importing multiple visualization libraries\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import mlab\n",
    "import pylab as pl\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# importing libraries to manipulate the data files\n",
    "import os\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# importing scientific python packages\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# import a library to read the .aiff format\n",
    "import aifc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `train` folder contains many `.aiff` files (2 second snippets) and we have `.csv` document which contains the corresponding labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the audio filenames\n",
    "filenames = sorted(glob(os.path.join('data','train','*.aiff')))\n",
    "print('There are '+str(len(filenames))+' files.' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the labels\n",
    "import pandas as pd\n",
    "labels = pd.read_csv(os.path.join('data','labels.csv'), index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the labels is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the file info\n",
    "whale_sample_file = 'train00006.aiff'\n",
    "whale_aiff = aifc.open(os.path.join('data','train',whale_sample_file),'r')\n",
    "print (\"Frames:\", whale_aiff.getnframes() )\n",
    "print (\"Frame rate (frames per second):\", whale_aiff.getframerate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the data\n",
    "whale_strSig = whale_aiff.readframes(whale_aiff.getnframes())\n",
    "whale_array = np.fromstring(whale_strSig, np.short).byteswap()\n",
    "plt.plot(whale_array)\n",
    "plt.xlabel('Frame number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal = whale_array.astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing a whale upcall in the notebook\n",
    "from IPython.display import Audio\n",
    "# Audio(signal, rate=3000, autoplay = True)# the rate is set to 3000 make the widget to run (seems the widget does not run with rate below 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working directly with the signals is hard (there is important frequency information). Let's calculate the spectrograms for each of the signals and use as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function for plotting spectrograms\n",
    "def PlotSpecgram(P, freqs, bins):\n",
    "    \"\"\"Spectrogram\"\"\"\n",
    "    Z = np.flipud(P) # flip rows so that top goes to bottom, bottom to top, etc.\n",
    "    xextent = 0, np.amax(bins)\n",
    "    xmin, xmax = xextent\n",
    "    extent = xmin, xmax, freqs[0], freqs[-1]\n",
    "    im = pl.imshow(Z, extent=extent,cmap = 'plasma')\n",
    "    pl.axis('auto')\n",
    "    pl.xlim([0.0, bins[-1]])\n",
    "    pl.ylim([0, freqs[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'NFFT':256, 'Fs':2000, 'noverlap':192}\n",
    "P, freqs, bins = mlab.specgram(whale_array, **params)\n",
    "PlotSpecgram(P, freqs, bins)\n",
    "plt.title('Spectrogram with an Upcall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will go through the files and extract the spectrograms from each of them. We will do it for the first N files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10000 #number of files to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary which contains all the spectrograms, labeled by the filename\n",
    "spec_dict = {}\n",
    "\n",
    "# threshold to cut higher frequencies\n",
    "m = 60\n",
    "\n",
    "# loop through all the files\n",
    "for filename in filenames[:N]:\n",
    "    # read the file\n",
    "    aiff = aifc.open(filename,'r')\n",
    "    whale_strSig = aiff.readframes(aiff.getnframes())\n",
    "    whale_array = np.fromstring(whale_strSig, np.short).byteswap()\n",
    "    # create the spectrogram\n",
    "    P, freqs, bins = mlab.specgram(whale_array, **params)\n",
    "    spec_dict[filename] = P[:m,:]\n",
    "\n",
    "# save the dimensions of the spectrogram\n",
    "spec_dim = P[:m,:].shape    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most machine learning algorithms in Python expect the data to come in a format **observations** x **features**. In order to get the data in this format we need to convert the two-dimensional spectrogram into a long vector. For that we will use the `ravel` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will put the data in a dictionary\n",
    "feature_dict = {}\n",
    "for key in filenames[:N]:\n",
    "    # vectorize the spectrogram\n",
    "    feature_dict[key.split('/')[-1]] = spec_dict[key].ravel()\n",
    "\n",
    "# convert to a pandas dataframe\n",
    "X = pd.DataFrame(feature_dict).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we do not need these objects anymore so let's release them from memory\n",
    "del feature_dict\n",
    "del spec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's save these variables for reuse\n",
    "np.save('X.npy',X)\n",
    "np.save('y.npy',np.array(labels['label'][X.index])[:N])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "https://www.kaggle.com/c/whale-detection-challenge\n",
    "\n",
    "https://github.com/jaimeps/whale-sound-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
