{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd373406-4226-4d81-bd20-5c3b4b672c58",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Data Access Methods\n",
    "\n",
    "**Marty Hidas**\n",
    "\n",
    "**Integrated Marine Observing System (IMOS) / Australian Ocean Data Network (AODN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e144fd67-cb73-42a4-b9f9-675805064985",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "This tutorial demostrates several ways data can be accessed remotely and loaded into a Python environment, including\n",
    "\n",
    "* OPeNDAP\n",
    "* OGC Web Feature Service (WFS)\n",
    "* direct access to files on cloud storage (AWS S3)\n",
    "* cloud-optimised formats Zarr & Parquet\n",
    "* New OGC APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbaa61b0-c4b0-4b10-be7b-e5f5d666ac29",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import all the tools we need...\n",
    "\n",
    "# For working with data\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "# For data that may be larger than the memory available on your computer...\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# For accessing OGC Web Feature Service\n",
    "from owslib.wfs import WebFeatureService\n",
    "\n",
    "# For accessing AWS S3 cloud storage\n",
    "import s3fs\n",
    "\n",
    "# Plotting tools\n",
    "import holoviews as hv\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "\n",
    "# For plotting geographic data & maps\n",
    "import geoviews as gv\n",
    "import geoviews.feature as gf\n",
    "from geoviews import opts\n",
    "from cartopy import crs\n",
    "\n",
    "# Use Matplotlib backend for slides web preview of notebook\n",
    "# Comment out these lines to get the default interactive plots using Bokeh\n",
    "hvplot.extension('matplotlib', compatibility='bokeh')\n",
    "gv.extension('matplotlib')\n",
    "gv.output(size=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96368746-0a9e-4b81-ae32-08823e379d67",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# The old school way\n",
    "\n",
    "The old (and still common) way to access data is to first download it to your computer and read it from there. \n",
    "This is easy for small datasets, but not always ideal:\n",
    "* What if the data is bigger than your hard disk?\n",
    "* What if you only need a small fraction of a dataset?\n",
    "* What if the dataset is routinely updated and you want to re-run your analysis on the latest data?\n",
    "* What if you want to run your analysis on another computer or in the cloud?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb4da0-e672-4ef1-beb0-b73db5eb5a6a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "These days it is often more convenient to have data managed in a central location and access it remotely.\n",
    "There are many ways this can be done. In this tutorial we will look at a few of the common ones, and some of the newer ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf83805-6e87-4fe4-95b2-1deabfab4380",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# OPeNDAP\n",
    "\n",
    "* [OPeNDAP](https://www.opendap.org/about) stands for \"Open-source Project for a Network Data Access Protocol\"\n",
    "* Provides access to metadata and data subsets via the Web without downloading an entire dataset\n",
    "* Many tools that can read NetCDF files can also talk to an OPeNDAP URL directly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfee5a9-28f4-4ef0-b471-de04a84e1333",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In Python, we can simply open the URL with `xarray`, then proceed with our analysis using the resulgting `Dataset` object.\n",
    "\n",
    "Here we use an example from the [AODN THREDDS server](https://thredds.aodn.org.au/thredds/catalog.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2f8f45-eabf-4e53-8076-e8d188e59d2e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up local data path\n",
    "import os\n",
    "TUTORIAL_BASEPATH = os.path.realpath('..')\n",
    "DATA_BASEPATH = os.path.join(os.path.dirname(TUTORIAL_BASEPATH), 'shared', 'IMOS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118639e-503d-4552-8b32-9ce5cc33535a",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Local copy of data file, in case server is overloaded...\n",
    "# opendap_url = os.path.join(DATA_BASEPATH,\n",
    "#                           \"IMOS_ANMN-NSW_TZ_20091029_PH100_FV02_TEMP-gridded-timeseries_END-20230316_C-20230520.nc\")\n",
    "# ds_mooring = xr.open_dataset(opendap_url)\n",
    "# ds_mooring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e9ca6c-f99f-481d-831f-8e13817e5e25",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "opendap_url = (\"https://thredds.aodn.org.au/thredds/dodsC/\"\n",
    "               \"IMOS/ANMN/NSW/PH100/gridded_timeseries/\"\n",
    "               \"IMOS_ANMN-NSW_TZ_20091029_PH100_FV02_TEMP-gridded-timeseries_END-20230316_C-20230520.nc\")\n",
    "\n",
    "ds_mooring = xr.open_dataset(opendap_url)\n",
    "ds_mooring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba40af7-cf45-4a09-a0d1-fadd5ec21fd6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(ds_mooring.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68891c3-0532-4e96-b4c8-cad9883b64cf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "This dataset is derived from repeated deployments of moored temperature loggers, binned to hourly intervals and interpolated to a fixed set of target depths. See the file metadata, or the associated [metadata record](https://catalogue-imos.aodn.org.au/geonetwork/srv/eng/catalog.search#/metadata/279a50e3-21a5-4590-85a0-71f963efab82) for more info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a789a-81e7-4725-a5e1-b124eda515df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hourly averages x 12 depths x 13+ yr = over a million points to plot!\n",
    "# Let's just look at a year's worth to speed things up...\n",
    "ds_mooring.sel(TIME=\"2022\").hvplot.scatter(x=\"TIME\", y=\"DEPTH\", c=\"TEMP\",\n",
    "                                           cmap=\"coolwarm\", alpha=0.2,\n",
    "                                           flip_yaxis=True, hover=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb6d7d-3110-459c-822f-1d613f883128",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ... or we can look at the full timeseries of temperature at a single depth\n",
    "ds_mooring.TEMP.sel(DEPTH=30).hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571909e3-71b4-427d-aefb-3d64dbd6158c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "# Web Feature Service (WFS)\n",
    "\n",
    "* A [standard](http://www.opengeospatial.org/standards/wfs) of the [Open Geospatial Consortium](http://www.opengeospatial.org/) (OGC)\n",
    "* Allows tabular geospatial data to be accessed via the Web.\n",
    "* A _feature_ has a _geometry_ (e.g. a point/line/polygon) indicating a geographic location, and a set of properties (e.g. temperature) \n",
    "* WFS allows filtering based on geometry or properties.\n",
    "* In Python WFS and other OGC Web Services (OWS) can be accessed using the [`owslib`](https://pypi.org/project/OWSLib/) library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1d1270-159f-46da-b6b8-b2162b0401d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "For example, most of the tabular data from the Australian Integrated Marine Observing System (IMOS) is available via WFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6250dfb4-fc51-4db7-9088-17bd2bd9e8c7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "wfs = WebFeatureService(url=\"https://geoserver-123.aodn.org.au/geoserver/wfs\",\n",
    "                        version=\"1.1.0\")\n",
    "wfs.identification.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af41cd03-3895-490a-8621-95a569a53a35",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Each dataset is served as a separate \"feature type\":\n",
    "print(f\"There are {len(wfs.contents)} fature types, e.g.\")\n",
    "list(wfs.contents)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08076cde-3bec-4199-932a-61ee43e8df64",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "For now we'll assume we already know which featuretype we want. In this example we'll look at a dataset containing condicutivity-temperature-depth (CTD) profiles obtained at the National Reference Stations around Australia ([here](https://catalogue-imos.aodn.org.au/geonetwork/srv/eng/catalog.search#/metadata/7b901002-b1dc-46c3-89f2-b4951cedca48)'s a detailed metadata record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f370e7-a79c-431e-9059-fee45837b0ef",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "typename = 'imos:anmn_ctd_profiles_data'\n",
    "wfs.get_schema(typename)['properties']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c02cee3-558e-4515-aaae-c5353f4c3f29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can read in a subset of the data by specifying a bounding box (in this case near Sydney, Australia).\n",
    "We'll get the result in CSV format so it's easy to read into a Pandas DataFrame.\n",
    "\n",
    "First we'll ask for just 10 features, for a quick look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee02762-2045-415f-b10c-c572b9e11899",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "xmin, xmax = 151.2, 151.25   # Port Hacking, near Sydney, NSW\n",
    "ymin, ymax = -34.2, -34.1\n",
    "\n",
    "response = wfs.getfeature(typename=typename,\n",
    "                          bbox=(xmin, ymin, xmax, ymax),\n",
    "                          maxfeatures=10,\n",
    "                          outputFormat='csv')\n",
    "df = pd.read_csv(response)\n",
    "response.close()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309726d2-a129-4fa2-a4ff-b450a0356384",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load local copy of CSV file returned...\n",
    "# local_csv = os.path.join(DATA_BASEPATH, 'wfs_response1.csv')\n",
    "# df = pd.read_csv(local_csv)\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bcbb6-605c-4aa9-8be1-8370a8d4b44e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We can also filter the data based on the values in specified columns (properties) and ask for only a subset of the columns to be returned. The filters need to be provided in XML format, but the `owslib` library allows us to construct them in a more Pythonic way.\n",
    "\n",
    "Here we select only the profiles associated with the Port Hacking 100m mooring site, and only the data points flagged as \"good data\" by automated quality-control procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1953a518-7a7f-4706-aedf-a1bc399fb4a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from owslib.etree import etree\n",
    "from owslib.fes import PropertyIsEqualTo, And\n",
    "\n",
    "filter = And([PropertyIsEqualTo(propertyname=\"site_code\", literal=\"PH100\"),\n",
    "              PropertyIsEqualTo(propertyname=\"PRES_REL_quality_control\", literal=\"1\"),\n",
    "              PropertyIsEqualTo(propertyname=\"TEMP_quality_control\", literal=\"1\"),\n",
    "              PropertyIsEqualTo(propertyname=\"PSAL_quality_control\", literal=\"1\"),\n",
    "              PropertyIsEqualTo(propertyname=\"CPHL_quality_control\", literal=\"1\")\n",
    "             ])\n",
    "filterxml = etree.tostring(filter.toXML(), encoding=\"unicode\")\n",
    "\n",
    "response = wfs.getfeature(typename=typename, filter=filterxml, outputFormat=\"csv\",\n",
    "                          propertyname=[\"TIME\", \"DEPTH\", \"TEMP\", \"PSAL\", \"CPHL\"]\n",
    "                         )\n",
    "df = pd.read_csv(response, parse_dates=[\"TIME\"])\n",
    "response.close()\n",
    "\n",
    "# the server adds a feature ID column we don't really need\n",
    "df.drop(columns='FID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcf91fe-1111-4313-9fef-b7650e351caf",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load local copy of CSV file returned...\n",
    "# local_csv = os.path.join(DATA_BASEPATH, 'wfs_response2.csv')\n",
    "# df = pd.read_csv(local_csv).drop(columns='FID')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b1860c-b573-4410-a45a-216abf9439e3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24705ca-8b5b-42a0-9b68-c482c66bae5e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can explore the temperature, salinity and chlorophyll profiles on by one\n",
    "temp_plot = df.hvplot(x=\"TEMP\", y=\"DEPTH\", groupby=\"TIME\", flip_yaxis=True, legend=False)\n",
    "psal_plot = df.hvplot(x=\"PSAL\", y=\"DEPTH\", groupby=\"TIME\", flip_yaxis=True, legend=False)\n",
    "cphl_plot = df.hvplot(x=\"CPHL\", y=\"DEPTH\", groupby=\"TIME\", flip_yaxis=True, legend=False)\n",
    "\n",
    "(temp_plot + psal_plot + cphl_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8ab58b-a465-4800-8172-c7b2cb7e4411",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can also extract the temperature measurements at a fixed depth\n",
    "# and compare to the timeseries from the mooring \n",
    "comp_depth = 20  # metres\n",
    "\n",
    "df_sub = df[df.DEPTH.round() == comp_depth]\n",
    "ctd_plot = df_sub.hvplot.scatter(x=\"TIME\", y=\"TEMP\", c=\"red\")\n",
    "\n",
    "mooring_plot = ds_mooring.TEMP.sel(DEPTH=comp_depth).hvplot()\n",
    "\n",
    "mooring_plot * ctd_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14889af6-212e-48d1-be86-636cf46a21c1",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "DRAFT"
    ]
   },
   "source": [
    "Further examples?\n",
    "* Plot timeseries of near-surface values\n",
    "* Plot profile by month of year?\n",
    "* Complute MLD (or read from `nrs_derived_indices_data`) and plot timeseries\n",
    "* Calculate average profile per month of year?\n",
    "* Plot timeseries of various phytoplankton species abundances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77bba9a-3fa1-43d0-984d-1532699d6423",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "TODO"
    ]
   },
   "source": [
    "**TODO** Add abstract & metadata link to the example WFS layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bae0efa-8466-421d-aace-39f5259cd695",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Direct access to files on cloud storage\n",
    "\n",
    "Data files made available to the public on cloud storage such as [Amazon S3](https://docs.aws.amazon.com/AmazonS3/latest/userguide/Welcome.html) can be accessed over the web as if they were stored locally. You just need to find the exact URL for each file.\n",
    "\n",
    "In Python, we can access S3 storage in a very similar way to a local filesystem using the `s3fs` library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fdff28-e958-42a8-8a07-43ebd5583020",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "For example, all the public data files hosted by the Australian Ocean Data Network are stored in an [S3 bucket](https://www.techtarget.com/searchaws/definition/AWS-bucket) called `imos-data`. You can browse the contents of the bucket and download individual files [here](https://imos-data.aodn.org.au). \n",
    "\n",
    "Below we'll look at a [high-resolution regional SST product](https://catalogue-imos.aodn.org.au/geonetwork/srv/eng/catalog.search#/metadata/a4170ca8-0942-4d13-bdb8-ad4718ce14bb) from IMOS (based on satellite and in-situ observations). This product is a collection of daily gridded NetCDF files covering the Australian region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b9e6e6-c345-46bf-a325-1f9e98059303",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3 = s3fs.S3FileSystem(anon=True)\n",
    "\n",
    "# List the most recent files available\n",
    "sst_files = s3.ls(\"imos-data/IMOS/SRS/SST/ghrsst/L4/RAMSSA/2023\")\n",
    "sst_files[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285652d9-de6c-48a8-917a-a7447a7c95b3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "DRAFT"
    ]
   },
   "outputs": [],
   "source": [
    "# Open the latest file and look at its contents\n",
    "ds = xr.open_dataset(s3.open(sst_files[-1]))\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9e48b0-e0d8-475b-ae1f-17bf90cfe9bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "DRAFT"
    ]
   },
   "outputs": [],
   "source": [
    "# Plot a subset of the dataset around Australia\n",
    "sst_var = 'analysed_sst'\n",
    "gds = gv.Dataset(ds.sel(lat=slice(-50, 0), lon=slice(105, 175)),\n",
    "                 kdims=['lon', 'lat'],\n",
    "                 vdims=[sst_var]\n",
    "                )\n",
    "sst_plot = (gds.to(gv.Image)\n",
    "               .opts(cmap='coolwarm', colorbar=True, aspect=1.4, title=ds.title))\n",
    "sst_plot * gf.land"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f9e6cf-4e07-42e2-ad13-fbb282f568d6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "It's worth understanding a little about how this works. \n",
    "\n",
    "The above example only makes use of the metadata from the file, one of the 4 data variables, and the `lon` and `lat` coordinates. On a local filesystem, it would be easy to read only these specific parts of the file from disk. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de52f75-90ca-49be-a644-5deb139077bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "However, on cloud storage services like S3 (also called \"object storage\") the basic read/write functions operate on the entire file (object), so at least in the backend, the entire file is read**. If you only need a small subset of a large file, this can be a very inefficient way to get it.\n",
    "\n",
    "** _Note: it is possible to request only a subset of an S3 object to be read, but this is more advanced usage than what we're doing here._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e731bc-985f-4d29-8647-8f0e2c476fe7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "DRAFT"
    ]
   },
   "source": [
    "For example, if we wanted to plot a timeseries of the above satellite SST product at a given point, we would only need a single value out of each file (corresponding to one point in the timeseries), but the entire file would need to be read each time.\n",
    "\n",
    "For a quick demo we'll try this with last month's files. `xarray` has a handy `open_mfdataset` function that can create a single `Dataset` object out of a series of files (with similar structure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac703357-680d-4ada-b536-93f81a3b3daf",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "DRAFT"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "s3_objs = [s3.open(f)\n",
    "           for f in s3.glob(\"imos-data/IMOS/SRS/SST/ghrsst/L4/RAMSSA/2023/202307*\")\n",
    "          ]\n",
    "mds = xr.open_mfdataset(s3_objs, engine=\"h5netcdf\")\n",
    "mds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec9f43c-ecf0-4334-951a-07df69c81d85",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The variables in the dataset are not loaded into memory (they're still `dask.array`s). However, in the background, each complete file had to be downloaded from S3 before the metadata needed by `open_mfdataset` could be read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab44ed1e-0c09-4ebb-a7bd-b85f7fd365e6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mds.analysed_sst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f1a88-a434-49e9-86d4-49b55aef83fd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's compare this to reading the same files from a local filesystem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d739cc-27ff-4678-a314-f1cafb177ab0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": [
     "DRAFT"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "from glob import glob\n",
    "local_files = glob(os.path.join(DATA_BASEPATH, \"RAMSSA\", \"*\"))\n",
    "\n",
    "mds = xr.open_mfdataset(local_files, engine=\"h5netcdf\")\n",
    "mds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3468213-a5a3-45c8-ab43-97d03b6b99df",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Whichever way we loaded the dataset, we can plot it the same way as any other `xarray.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42039a2-691e-4f9f-b936-f9d252215b17",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "mds[sst_var].sel(lat=-42, lon=150, method=\"nearest\").hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c89076b-480b-4a04-b7ca-c582786879fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Zarr - a cloud-optimised data format\n",
    "\n",
    "Zarr is a relatively new data format specifically developed for efficient access to multi-dimensional data in the cloud. Each dataset is broken up into many smaller files containing \"chunks\" of the data, organised in a standard hierarchy. The metadata are stored in separate files. When reading such a dataset, only the required information is read for each operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0aabac-3b27-4a93-8c68-8d8004c719e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A Zarr \"store\" can easily be opened as an xarray.Dataset\n",
    "\n",
    "# In this case the Zarr store is in an S3 bucket\n",
    "store = s3fs.S3Map(root='imos-data-pixeldrill/zarrs/2021/', s3=s3, check=False)\n",
    "\n",
    "zds = xr.open_zarr(store)\n",
    "zds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9360766-1fd6-49a2-9825-1009ea139e76",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can see the chunked structure of the data by looking at one of the variables\n",
    "zds.sea_surface_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65ac6cc-3d98-449e-a50f-17c1a22715d0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": [
     "DRAFT"
    ]
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# We can plot this dataset in exactly the same way as the NetCDF-based one\n",
    "sst_var = 'sea_surface_temperature'\n",
    "gds = gv.Dataset(zds[sst_var].sel(time='2021-01-02', lat=slice(0, -50), lon=slice(105, 175)),\n",
    "                 kdims=['lon', 'lat'],\n",
    "                 vdims=[sst_var]\n",
    "                )\n",
    "sst_plot = (gds.to(gv.Image, ['lon', 'lat'])\n",
    "               .opts(cmap='coolwarm', colorbar=True, aspect=1.4, title=zds.title))\n",
    "sst_plot * gf.land"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed83c49-cf11-4306-a4ee-d058a6b417d4",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "A more detailed example of working with similar data in Zarr format can be found here: https://github.com/aodn/rimrep-examples/blob/main/Python_based_scripts/zarr.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66e306b-1fe7-4083-bcb1-429325eae669",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Parquet\n",
    "\n",
    "* Parquet is a cloud-optimised format designed for tabular data.\n",
    "* Each column of the table is stored in a separate file/object.\n",
    "* These can be further partitioned into _row groups_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be833434-9d83-45fd-8786-a50fc35969b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "For a quick demo, we'll borrow an example from [this more detailed notebook](https://github.com/aodn/rimrep-examples/blob/main/Python_based_scripts/Extracting_Water_Temperature_at_Site.ipynb), looking at temperature logger data from the Australian Institute of Marine Science. The dataset contains 150 million temperature measurements from numerous sites around Australia ([metadata for this dataset](https://apps.aims.gov.au/metadata/view/4a12a8c0-c573-11dc-b99b-00008a07204e))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42830aee-9afa-4cfe-811c-b70f6d13028d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Here's the path to the dataset on AWS S3\n",
    "parquet_path = \"s3://rimrep-data-public/091-aims-sst/test-50-64-spatialpart/\"\n",
    "\n",
    "# Let's see if there are any temperature loggers near us (in Dunsborough, Western Australia)\n",
    "filters = [('lon', '>', 114.5),\n",
    "          ('lon', '<', 115.5),\n",
    "          ('lat', '>', -34.),\n",
    "          ('lat', '<', -33.)]\n",
    "\n",
    "df = dd.read_parquet(parquet_path,\n",
    "                     filters=filters,\n",
    "                     # only read the site names and QC'd temperature values\n",
    "                     columns = ['site', 'qc_val'],\n",
    "                     index='time',\n",
    "                     storage_options = {\"anon\": True}\n",
    "                    )\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a58bb0-6d9d-4b84-9ec2-677792a2a25f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2f66ff-fe9b-4218-b7bc-da31614478eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We still have 1.6 million values.\n",
    "# Let's see how many sites we have...\n",
    "df.site.unique().compute()  # need compute() for dask to give us an answer now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7bea2d-e1e8-4a2b-8cbd-3dba7f5521a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot a timeseries for the Geographe Bay site\n",
    "df_local = df[df.site == \"Geographe Bay\"].compute()\n",
    "df_local.hvplot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4f51b-2293-4d91-a3fc-48d7fcdbee7e",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "#### Alternative dataset\n",
    "\n",
    "Another Parquet example from OBIS is shown in [this notebook](https://github.com/MathewBiddle/globe/blob/main/organismQuantity_check.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aa13e5-5f12-4895-936a-3dff24cf4819",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "TODO",
     "DRAFT"
    ]
   },
   "source": [
    "# Other methods\n",
    "\n",
    "* ERDDAP\n",
    "* New OGC API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdc285e-7373-4105-8b57-88b980bfb582",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": [
     "TODO"
    ]
   },
   "source": [
    "# TODO\n",
    "\n",
    "- [ ] Add metadata links for datasets used\n",
    "- [ ] Find alternative data sources in other regions (at least US)\n",
    "- [ ] Provide sample data to store on JupyterHub for local access\n",
    "- [ ] Provide clear instructions for participants which data to access!\n",
    "- [ ] Acknowledge previous tutorial and other sources...\n",
    "- [ ] Data acknowledgements / citations?\n",
    "- [ ] Spell check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d339abfd-647f-47ef-bbdc-4877d76e2bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OHW ipykernel",
   "language": "python",
   "name": "ohw-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
